{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f023995",
   "metadata": {},
   "source": [
    "# Evaluate AffectNet March2021\n",
    "* Author: Sungguk Cha\n",
    "* eMail: sungguk@ncsoft.com\n",
    "* Date: 4th Nov. 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd3275",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411264c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet101, mobilenet_v2\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "\n",
    "from robust_optimization import RobustOptimizer\n",
    "\n",
    "print(f'Torch: {torch.__version__}')\n",
    "print(f'Timm: {timm.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a8407",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "affectnet_dir = './data_eila_ft_test/'\n",
    "USE_ENET2=False #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dfe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 32 #48# 32# 32 #16 #8 #\n",
    "epochs = 40\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "seed = 42\n",
    "device = 'cuda'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=260 if USE_ENET2 else 224 # 300 # 80 #\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "print(test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Changing EiLA dataset "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "593914f653f703dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read the files in the ./data directory and create a csv file with paths and labels\n",
    "#create a df\n",
    "df = pd.DataFrame(columns=['phase','img_path','label'])\n",
    "for i in range(7):\n",
    "    file_names = os.listdir(f'../../EiLA_data/val_set/{i}')\n",
    "    for file_name in file_names:\n",
    "        # only if .ds_store is not in the file_name\n",
    "        if '.DS_Store' not in file_name:\n",
    "            df.loc[len(df)] = {'phase': 'val', 'img_path': f'{i}/{file_name}', 'label': i} #./data/val/{i}/{file_name}\n",
    "        \n",
    "df.to_csv('../../EiLA_data/affectnet_val.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3e7f79fe809aa1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27dfe0420717a659",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6af7d729",
   "metadata": {},
   "source": [
    "## AffectNet Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://github.com/yaoing/DAN/blob/main/affectnet.py\n",
    "# phase: one of ['train', 'val']\n",
    "class AffectNet(data.Dataset):\n",
    "    def __init__(self, aff_path, phase, use_cache=True, transforms=None, force=False):\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "        self.aff_path = aff_path\n",
    "        self.base_path = os.path.join(self.aff_path, f'{self.phase}/')\n",
    "        \n",
    "        if use_cache:\n",
    "            cache_path = os.path.join(aff_path,f'eila.csv')\n",
    "            if os.path.exists(cache_path) and not force:\n",
    "                df = pd.read_csv(cache_path)\n",
    "            else:\n",
    "                df = self.get_df()\n",
    "                df.to_csv(cache_path)\n",
    "        else:\n",
    "            df = self.get_df()\n",
    "\n",
    "        self.data = df[df['phase'] == phase]\n",
    "\n",
    "        self.file_paths = self.data.loc[:, 'img_path'].values\n",
    "        self.label = self.data.loc[:, 'label'].values\n",
    "\n",
    "        self.emotion_labels=['Neutral','Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
    "        sample_label, sample_counts = np.unique(self.label, return_counts=True)\n",
    "        for l, c in zip(sample_label, sample_counts):\n",
    "            print(f'{self.emotion_labels[l]}: {c} ', end='')\n",
    "        print(f'\\n{len(self)} images')\n",
    "\n",
    "    def get_df(self):\n",
    "        base_path = os.path.join(self.aff_path, f'{self.phase}_set/')\n",
    "        self.base_path = base_path\n",
    "        data = []\n",
    "        \n",
    "        for anno in glob.glob(base_path + 'annotations/*_exp.npy'):\n",
    "            idx = os.path.basename(anno).split('_')[0]\n",
    "            img_path = f'images/{idx}.jpg'\n",
    "            label = int(np.load(anno))\n",
    "            data.append([self.phase,img_path,label])\n",
    "        \n",
    "        return pd.DataFrame(data = data,columns = ['phase','img_path','label'])\n",
    "    \n",
    "    def get_weight(self):\n",
    "        self.emotion_labels=['Neutral','Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        for i, emotion in enumerate(self.emotion_labels):\n",
    "            self.class_to_idx[emotion] = i\n",
    "            self.idx_to_class[i] = emotion\n",
    "        sample_label, sample_counts = np.unique(self.label, return_counts=True)\n",
    "        for l, c in zip(sample_label, sample_counts):\n",
    "            print(f'{self.emotion_labels[l]}: {c} ', end='')\n",
    "        print('')\n",
    "        \n",
    "        cw = 1/sample_counts\n",
    "        cw /= cw.min()\n",
    "        class_weights = {i:cwi for i, cwi in zip(sample_label, cw)}\n",
    "        print(class_weights)\n",
    "        return class_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.base_path, self.file_paths[idx])\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        label = self.label[idx]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09579cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = AffectNet(affectnet_dir, 'train', transforms=train_transforms, force=False)\n",
    "valset = AffectNet(affectnet_dir, 'val', transforms=test_transforms, force=False)\n",
    "# trainloader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "valloader = data.DataLoader(valset, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd4f957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weights = valset.get_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fb636",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba12a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "def set_parameter_requires_grad(model, requires_grad):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1979cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "weights = torch.FloatTensor(list(class_weights.values())).to(device)\n",
    "\n",
    "def label_smooth(target, n_classes: int, label_smoothing=0.1):\n",
    "    # convert to one-hot\n",
    "    batch_size = target.size(0)\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    soft_target = torch.zeros((batch_size, n_classes), device=target.device)\n",
    "    soft_target.scatter_(1, target, 1)\n",
    "    # label smoothing\n",
    "    soft_target = soft_target * (1 - label_smoothing) + label_smoothing / n_classes\n",
    "    return soft_target\n",
    "\n",
    "def cross_entropy_loss_with_soft_target(pred, soft_target):\n",
    "    #logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "    return torch.mean(torch.sum(- weights*soft_target * torch.nn.functional.log_softmax(pred, -1), 1))\n",
    "\n",
    "def cross_entropy_with_label_smoothing(pred, target):\n",
    "    soft_target = label_smooth(target, pred.size(1)) #num_classes) #\n",
    "    return cross_entropy_loss_with_soft_target(pred, soft_target)\n",
    "\n",
    "criterion=cross_entropy_with_label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "# models.append(('affectnet_FT_gpt_200epochs.pt', '../../models/affectnet_emotions/affectnet_FT_gpt_200epochs.pt'))\n",
    "# models.append(('EfficientNet_b0_best_afew', '../../models/affectnet_emotions/enet_b0_8_best_afew.pt'))\n",
    "# models.append(('EfficientNet_b0_best_vgaf', '../../models/affectnet_emotions/enet_b0_8_best_vgaf.pt'))\n",
    "# models.append(('affectnet_vggface2_rexnet150','../../models/affectnet_emotions/enet_b0_8_va_mtl.pt'))\n",
    "# models.append(('enet_b2_best','../../models/affectnet_emotions/enet_b2_8.pt'))\n",
    "models.append(('affectnet_FT_1 is fine-tune version of enet_b2_best','../../models/affectnet_emotions/affectnet_FT_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6076af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_8 = {0: 'Anger', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happiness', 5: 'Neutral', 6: 'Sadness', 7: 'Surprise'}\n",
    "new_order_8 = ['Neutral','Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']\n",
    "new_order_8 = {k: new_order_8.index(v) for k, v in pretrained_8.items()}\n",
    "print(new_order_8)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_pretrained_7(model, length, dataloader, criterion, device):\n",
    "    pretrained_8 = {0: 'Anger', 1: 'Disgust', 2: 'Fear', 3: 'Happiness', 4: 'Neutral', 5: 'Sadness', 6: 'Surprise'}\n",
    "    new_order_8 = ['Neutral', 'Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
    "    new_order_8 = {k: new_order_8.index(v) for k, v in pretrained_8.items()}\n",
    "    new_order = new_order_8\n",
    "    print(new_order)\n",
    "\n",
    "    reversed_order = {v: k for k, v in new_order.items()}\n",
    "    reordered_labels = [pretrained_8[reversed_order[i]] for i in range(len(pretrained_8))]\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)  # Move model to the appropriate device (MPS or CPU)\n",
    "\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for (images, emotions) in tqdm(dataloader):\n",
    "        images = images.to(device)  # Move images to the appropriate device (MPS or CPU)\n",
    "        emotions = emotions.to(device)  # Ensure emotions are also on the correct device\n",
    "\n",
    "        preds = model(images)\n",
    "\n",
    "        # accuracy\n",
    "        preds = torch.concat([preds[:, 0:1], preds[:, 2:]], dim=1)  # Concatenating predictions\n",
    "        preds = torch.argmax(preds, dim=1).cpu()  # Moving predictions back to CPU to apply new_order\n",
    "        preds = preds.apply_(new_order.get)  # Apply new order to map emotions\n",
    "\n",
    "        acc = torch.eq(preds, emotions.cpu()).sum()  # Move emotions to CPU for comparison\n",
    "        accuracy += acc\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())  # Collect predictions for confusion matrix\n",
    "        all_labels.extend(emotions.cpu().numpy())  # Collect true labels for confusion matrix\n",
    "\n",
    "    loss /= length\n",
    "    accuracy /= length #(length - 499)  # Accuracy calculation\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Normalize confusion matrix to get percentages\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Print accuracy and loss\n",
    "    print(f'Accuracy: {accuracy:.4f}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Plot the confusion matrix with percentages\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=reordered_labels, yticklabels=reordered_labels)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_title('Confusion Matrix with Percentages')\n",
    "    \n",
    "    #save the cm \n",
    "    plt.savefig('cm.png')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate F1-scores and balanced accuracy\n",
    "    f1_scores = f1_score(all_labels, all_preds, average=None)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(\"F1-scores per class:\", f1_scores)\n",
    "    print(\"Macro F1-score: %.4f\" % macro_f1)\n",
    "\n",
    "\n",
    "    # Calculate weighted accuracy (balanced accuracy)\n",
    "    matrix = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize confusion matrix\n",
    "    class_recalls = np.diag(matrix) / np.sum(matrix, axis=1)  # Recall per class\n",
    "    balanced_accuracy = np.mean(class_recalls)  # Weighted accuracy\n",
    "    print(\"Weighted accuracy: %.4f\" % balanced_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "295a13edac76d00d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path, valloader, valset):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # Choose device: MPS or CPU\n",
    "\n",
    "    model = torch.load(model_path, map_location=device)  # Load model and move to appropriate device\n",
    "    model = model.eval().to(device)  # Set model to evaluation mode and move to device\n",
    "\n",
    "    eval_pretrained_7(model, len(valset), valloader, criterion=None, device=device)  # Pass device to eval_pretrained_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5dc4ddcf1fcf7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    print(name)\n",
    "    test(model, valloader,valset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
